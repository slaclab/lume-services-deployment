{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# tracking uri\u001b[39;00m\n\u001b[0;32m     13\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLFLOW_TRACKING_URI\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://ard-mlflow.slac.stanford.edu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241m.\u001b[39mset_experiment(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# set the experiment name here\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MlflowClient\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_app_client\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mlflow' is not defined"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "\n",
    "# dont do this in production\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"admin\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"password\"\n",
    "os.environ[\"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\"] = \"true\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"eu-west-3\"\n",
    "os.environ[\"AWS_REGION\"] = \"eu-west-3\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"admin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"test_password\"\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://172.24.5.222:9020\"\n",
    "# tracking uri\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"https://ard-mlflow.slac.stanford.edu\"\n",
    "\n",
    "\n",
    "\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "from mlflow.server import get_app_client\n",
    "import mlflow.pytorch\n",
    "import mlflow.keras\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "mlflow.set_experiment(\"my-experiment\")\n",
    "\n",
    "data_X = np.random.uniform(-1, 1, (1000, 2))\n",
    "data_y = np.max(data_X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic \n",
    "\n",
    "class MyModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    # this function is called when the model is loaded using pyfunc.load_model\n",
    "    def predict(self, context, input, **kwargs):\n",
    "        return self.predict_internal(input, **kwargs)\n",
    "\n",
    "    # this function is the true predic function, but we need the context parameter to be able to use the model with mlflow\n",
    "    def predict_internal(self, input, **kwargs):\n",
    "        return np.max(input, axis=1)\n",
    "\n",
    "    def inverse_predict_internal(self, input, **kwargs):\n",
    "        return np.sqrt(input)\n",
    "\n",
    "    def save_model(self):\n",
    "        with open(f\"{self.model_name}.txt\", \"w\") as f:\n",
    "            f.write(\"model saved\")\n",
    "\n",
    "    def load_model(self):\n",
    "        with open(f\"{self.model_name}.txt\", \"r\") as f:\n",
    "            return f.read()\n",
    "        \n",
    "    def unwrap_python_model(self):\n",
    "        return self.lume_model\n",
    "    \n",
    "    def make_serializable(self, model):\n",
    "        return model\n",
    "    \n",
    "model = MyModel(\"model1\")\n",
    "\n",
    "with mlflow.start_run() as run:  # you can use run_name=\"test1\" to give a name to the run otherwise it will a random name\n",
    "    \n",
    "\n",
    "    model = MyModel(\"model1\")\n",
    "    input_sample = np.random.uniform(-1, 1, (100, 2))\n",
    "    # model.save_model() # no need to save the model since it is saved in log_model\n",
    "    mlflow.log_param(\"model_name\", model.model_name)\n",
    "    mlflow.log_param(\"dummy_param1\", \"dummy_value1\")\n",
    "    mlflow.log_param(\"dummy_param2\", 0.33)\n",
    "    for i in range(10):        \n",
    "        mlflow.log_metric(\"metric1\", (i / 10) ** 2 , step=i)\n",
    "        mlflow.log_metric(\"metric2\", (i / 10) ** 3 , step=i)\n",
    "        mlflow.log_metric(\"loss\", (1 / (i + 0.1) + np.random.normal(0, 0.1)) , step=i)\n",
    "\n",
    "    # lets make some pretty graphs to store\n",
    "\n",
    "    graph = plt.figure()\n",
    "    plt.plot(range(100), [(i / 10) ** 2 for i in range(100)])\n",
    "    mlflow.log_figure(graph, \"figures/metric1.png\")\n",
    "\n",
    "    # alternative way to log a figure\n",
    "    graph = plt.figure()\n",
    "    plt.plot(range(100), [(i / 10) ** 3 for i in range(100)])\n",
    "    graph.savefig(\"metric2.png\")\n",
    "    mlflow.log_artifact(\"metric2.png\", artifact_path=\"figures\")\n",
    "    \n",
    "    \n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model_files\",\n",
    "        python_model=model,\n",
    "        signature=infer_signature(input_sample, model.predict_internal(input_sample)),\n",
    "        input_example=input_sample,\n",
    "        # registered_model_name=\"model1\",  # this will automatically register the model and iterate the version\n",
    "     )\n",
    "\n",
    "    # if you wanna log the model without the wrapper\n",
    "    model.save_model()\n",
    "    mlflow.log_artifact(\n",
    "        f\"{model.model_name}.txt\", artifact_path=\"model_files_no_mlflow\"\n",
    "    )\n",
    "\n",
    "    # set some tags\n",
    "    mlflow.set_tag(\"tag1\", \"tag_value1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/21 15:15:10 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of keras. If you encounter errors during autologging, try upgrading / downgrading keras to a supported version, or try upgrading MLflow.\n",
      "2024/03/21 15:15:10 WARNING mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics because creating `GPUMonitor` failed with error: Failed to initialize NVML, skip logging GPU metrics: Insufficient Permissions.\n",
      "2024/03/21 15:15:10 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "2024/03/21 15:15:11 WARNING mlflow.keras.autologging: Unrecognized dataset type <class 'list'>. Dataset logging skipped.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1169\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0050 \n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.6992e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2586e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1484e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1282e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5761e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.2880e-05 \n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.9213e-05\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/21 15:15:46 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/03/21 15:15:46 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "# keras model\n",
    "\n",
    "inputs = [keras.Input(name=\"input1\", shape=(1,)), keras.Input(name=\"input2\", shape=(1,))]\n",
    "x = keras.layers.concatenate(inputs)\n",
    "x1 = keras.layers.Dense(64, activation='relu')(x)\n",
    "x2 = keras.layers.Dense(64, activation='relu')(x1)\n",
    "outputs = keras.layers.Dense(1, name=\"output\")(x2)\n",
    "model_keras = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "mlflow.keras.autolog(log_models=False)\n",
    "with mlflow.start_run() as run:\n",
    "    model_keras.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model_keras.fit([data_X[:, 0], data_X[:, 1]], data_y, epochs=10)\n",
    "    mlflow.set_tag(\"tag1\", \"tag_value1\")\n",
    "    signature = infer_signature(data_X, model_keras.predict([data_X[:, 0], data_X[:, 1]]))\n",
    "    model_info_keras = mlflow.keras.log_model(model_keras, \"keras_model\", signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/21 15:15:46 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n",
      "2024/03/21 15:15:46 WARNING mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics because creating `GPUMonitor` failed with error: Failed to initialize NVML, skip logging GPU metrics: Insufficient Permissions.\n",
      "2024/03/21 15:15:46 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "c:\\Users\\gbm96348\\AppData\\Local\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6110944151878357\n",
      "1 0.22628964483737946\n",
      "2 0.38499715924263\n",
      "3 0.315997451543808\n",
      "4 0.23798348009586334\n",
      "5 0.22690147161483765\n",
      "6 0.2467193454504013\n",
      "7 0.263405442237854\n",
      "8 0.2664637565612793\n",
      "9 0.25781917572021484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/21 15:16:08 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/03/21 15:16:08 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# torch model \n",
    "model_torch = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 64),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    "    torch.nn.Linear(1, 1)\n",
    ")\n",
    "\n",
    "mlflow.pytorch.autolog()\n",
    "with mlflow.start_run() as run:\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_torch.parameters(), lr=0.01)#\n",
    "    batch_size = 1000\n",
    "    # log params\n",
    "    mlflow.log_param(\"lr\", 0.01)\n",
    "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "    mlflow.log_param(\"loss\", \"MSELoss\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    \n",
    "    for t in range(10):\n",
    "        losses = []\n",
    "        for i in range(0, len(data_X), batch_size):\n",
    "            X = torch.tensor(data_X[i:i+batch_size], dtype=torch.float32)\n",
    "            y = torch.tensor(data_y[i:i+batch_size], dtype=torch.float32)\n",
    "            y_pred = model_torch(X)\n",
    "            \n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        mlflow.log_metric(\"loss\", np.mean(losses), step=t)\n",
    "        print(t, np.mean(losses))\n",
    "    \n",
    "    mlflow.set_tag(\"tag1\", \"tag_value1\")\n",
    "        \n",
    "        \n",
    "    signature = infer_signature(data_X, model_torch(torch.tensor(data_X, dtype=torch.float32)).detach().numpy()) # optional but useful\n",
    "\n",
    "    model_info_torch = mlflow.pytorch.log_model(model_torch, \"torch_model\", signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs:/4ccf7c06ae434129a52d9ce17a0b09fb/model_files\n",
      "runs:/4d235f583ba5476b9e4e37e1f4b6ddf0/keras_model\n",
      "runs:/5c80c639dca64dfc9e624b283e65bb7d/torch_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/21 15:16:26 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: generic_model, version 19\n",
      "2024/03/21 15:16:27 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: keras_model, version 18\n",
      "2024/03/21 15:16:27 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: torch_model, version 18\n"
     ]
    }
   ],
   "source": [
    "# specifc model uris\n",
    "print(model_info.model_uri)\n",
    "print(model_info_keras.model_uri)\n",
    "print(model_info_torch.model_uri)\n",
    "\n",
    "# lets register the models\n",
    "client = MlflowClient()\n",
    "try:\n",
    "    client.create_registered_model(\"generic_model\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    client.create_registered_model(\"keras_model\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    client.create_registered_model(\"torch_model\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# register the models\n",
    "result_generic = client.create_model_version(\n",
    "    name=\"generic_model\",\n",
    "    source=model_info.model_uri,\n",
    "    run_id=model_info.run_id,\n",
    ")\n",
    "\n",
    "\n",
    "result_keras = client.create_model_version(\n",
    "    name=\"keras_model\",\n",
    "    source=model_info_keras.model_uri,\n",
    "    run_id=model_info_keras.run_id,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "result_torch = client.create_model_version(\n",
    "    name=\"torch_model\",\n",
    "    source=model_info_torch.model_uri,\n",
    "    run_id=model_info_torch.run_id,\n",
    ")\n",
    "\n",
    "\n",
    "client.set_registered_model_alias(\"generic_model\", \"champion\", result_generic.version)\n",
    "client.set_registered_model_alias(\"keras_model\", \"champion\", result_keras.version)\n",
    "client.set_registered_model_alias(\"torch_model\", \"champion\", result_torch.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]2024/03/21 15:16:28 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n",
      "Downloading artifacts: 100%|██████████| 11/11 [00:00<00:00, 154.74it/s]\n",
      "Downloading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]2024/03/21 15:16:29 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n",
      "Downloading artifacts: 100%|██████████| 10/10 [00:00<00:00, 161.29it/s]\n",
      "Downloading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]2024/03/21 15:16:30 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n",
      "Downloading artifacts: 100%|██████████| 10/10 [00:00<00:00, 250.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generic model\n",
      "mlflow.pyfunc.loaded_model:\n",
      "  artifact_path: model_files\n",
      "  flavor: mlflow.pyfunc.model\n",
      "  run_id: 4ccf7c06ae434129a52d9ce17a0b09fb\n",
      " <class 'mlflow.pyfunc.PyFuncModel'>\n",
      "Keras model\n",
      "<Functional name=functional_3, built=True> <class 'keras.src.models.functional.Functional'>\n",
      "Torch model\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "  (1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (4): Linear(in_features=1, out_features=1, bias=True)\n",
      ") <class 'torch.nn.modules.container.Sequential'>\n",
      "[[-0.6133544   0.1831888 ]\n",
      " [-0.2029367   0.40861749]]\n",
      "Generic model\n",
      "[0.1831888  0.40861749]\n",
      "Keras model\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "[[0.18560304]\n",
      " [0.41245562]]\n",
      "Torch model\n",
      "[[0.17965901]\n",
      " [0.16728517]]\n"
     ]
    }
   ],
   "source": [
    "# get model ids from registered models\n",
    "genric_model  = mlflow.pyfunc.load_model(f\"models:/generic_model@champion\")\n",
    "keras_model = mlflow.keras.load_model(f\"models:/keras_model@champion\")\n",
    "torch_model = mlflow.pytorch.load_model(f\"models:/torch_model@champion\")\n",
    "\n",
    "# get model info\n",
    "print(\"Generic model\")\n",
    "print(genric_model, type(genric_model))\n",
    "print(\"Keras model\")\n",
    "print(keras_model, type(keras_model))\n",
    "print(\"Torch model\")\n",
    "print(torch_model, type(torch_model))\n",
    "\n",
    "new_data = np.random.uniform(-1, 1, (2, 2))\n",
    "print(new_data)\n",
    "\n",
    "# lets run some predictions\n",
    "\n",
    "print(\"Generic model\")\n",
    "print(genric_model.predict(new_data))\n",
    "print(\"Keras model\")\n",
    "print(keras_model.predict([new_data[:, 0], new_data[:, 1]]))\n",
    "print(\"Torch model\")\n",
    "print(torch_model(torch.tensor(new_data, dtype=torch.float32)).detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output1': [0.1661815345287323, 0.16083329916000366, 0.15006259083747864]}\n"
     ]
    }
   ],
   "source": [
    "# We can create a wrapper for these models into lume-models\n",
    "from lume_model.base import LUMEBaseModel\n",
    "from lume_model.variables import ScalarInputVariable, ScalarOutputVariable\n",
    "import torch\n",
    "import keras\n",
    "\n",
    "class ExampleModel(LUMEBaseModel):\n",
    "    model: torch.nn.modules.container.Sequential\n",
    "    \n",
    "    # keras model\n",
    "    # model: keras.Model\n",
    "    # def evaluate(self, input_dict):\n",
    "\n",
    "    #     input1 = np.array(input_dict[\"input1\"])\n",
    "    #     input2 = np.array(input_dict[\"input2\"])\n",
    "    #     output = self.model_predict([input1, input2])\n",
    "    #     output_dict = {\"output1\": output}\n",
    "    #     return output_dict\n",
    "    \n",
    "    # generic model\n",
    "    # model: mlflow.pyfunc.PyFuncModel\n",
    "    # def evaluate(self, input_dict):\n",
    "    #     # input_dict is a dictionary with the input variable names as keys\n",
    "    #     input = np.array([input_dict[\"input1\"], input_dict[\"input2\"]])\n",
    "    #     input = input.reshape(len(input_dict[\"input1\"]), -1)\n",
    "    #     output = self.model.predict(input)\n",
    "    #     output = output.reshape(-1).tolist()\n",
    "    #     output_dict = {\"output1\": output}\n",
    "    #     return output_dict\n",
    "\n",
    "    # torch model\n",
    "    model: torch.nn.modules.container.Sequential\n",
    "    def evaluate(self, input_dict):\n",
    "        input = torch.tensor([input_dict[\"input1\"], input_dict[\"input2\"]], dtype=torch.float32)\n",
    "        input = input.reshape(len(input_dict[\"input1\"]), -1)\n",
    "        output = self.model(input)\n",
    "        output = output.detach().numpy().reshape(-1).tolist()\n",
    "        output_dict = {\"output1\": output}\n",
    "        return output_dict\n",
    "    \n",
    "input_variables = [\n",
    "    ScalarInputVariable(name=\"input1\", default=0.1, value_range=[0.0, 1.0]),\n",
    "    ScalarInputVariable(name=\"input2\", default=0.2, value_range=[0.0, 1.0]),\n",
    "]\n",
    "output_variables = [\n",
    "    ScalarOutputVariable(name=\"output1\"),\n",
    "]\n",
    "\n",
    "m = ExampleModel(input_variables=input_variables, output_variables=output_variables, model=torch_model)\n",
    "\n",
    "input_dict = {\n",
    "    \"input1\": [0.3,0.2,-0.2],\n",
    "    \"input2\": [0.1,0.16,-0.1],\n",
    "}\n",
    "print(m.evaluate(input_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
